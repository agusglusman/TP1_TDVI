---
title: "Tp1-TD6-Fuchs-Olivera-Glusman"
author: "Carolina Olivera, Milena Fuchs, Agustina Glusman"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r}
#install.packages("recipes") #Descomentar si no lo tienen ya instalado.
library(rpart)
library(ggplot2)
library(dplyr)
library(janitor)
library(caret)
```

### Ejercicio 1

**1- Introducción al problema seleccionado**

**Problema a resolver**

El conjunto de datos seleccionado es `loan_data.csv`, que contiene 45.000 registros y 14 variables relacionadas con solicitantes de préstamos. La información tiene características demográficas, financieras y crediticias de los individuos junto con detalles específicos de los préstamos. La variable objetivo es `loan_status`, que indica si un crédito fue aprobado (1) o rechazado (0), generando un problema de **clasificación binaria**, es decir, predecir si un préstamo se aprueba o se rechaza según el perfil de la persona y del crédito.

Dentro de las variables principales se incluyen:

-   **Demográficas:** `person_age`, `person_gender`, `person_education`, `person_emp_exp`, `person_home_ownership`.\
-   **Financieras:** `person_income`, `credit_score`, `cb_person_cred_hist_length`, `previous_loan_defaults_on_file`.\
-   **Del préstamo:** `loan_amnt`, `loan_int_rate`, `loan_percent_income`, `loan_intent`.

En este trabajo utilizamos árboles de decisión como modelo principal, ya que resultan apropiados para problemas de clasificación y permiten reconocer de forma clara qué factores están más asociados a la aprobación o rechazo de un préstamo. Además, estos pueden trabajar con distintos tipos de variables, tanto numéricas como categóricas, lo que los hace una herramienta flexible y adecuada para este tipo de análisis.

**Origen del dataset:** <https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data>

### Ejercicio 2

**Preparación de los datos**

En esta sección cargamos el dataset `loan_data.csv`, realizamos un preprocesamiento mínimo y un análisis exploratorio que incluye estadísticas descriptivas, visualizaciones y comentarios sobre las principales características observadas.

```{r setup, include=FALSE}
datos <- read.csv('loan_data.csv', stringsAsFactors = FALSE)


# Preprocesamiento
datos$loan_status <- factor(datos$loan_status,
                           levels = c("0","1"),
                           labels = c("Rechazado","Aprobado"))

datos$person_gender <- as.factor(datos$person_gender)
datos$person_education <- as.factor(datos$person_education)
datos$person_home_ownership <- as.factor(datos$person_home_ownership)
datos$loan_intent <- as.factor(datos$loan_intent)
datos$previous_loan_defaults_on_file <- as.factor(datos$previous_loan_defaults_on_file)

# Chequeo de faltantes
colSums(is.na(datos))

```

El chequeo de valores faltantes mostró que ninguna variable presenta datos ausentes, por lo que se puede trabajar directamente con el dataset completo sin aplicar técnicas de imputación.

```{r setup, include=FALSE}

# 2.3 Estadísticas descriptivas
# Numéricas principales
summary(datos[, c("person_age","person_income","loan_amnt","loan_int_rate","credit_score")])

# Categóricas principales
table(datos$loan_status)
prop.table(table(datos$loan_status))
table(datos$loan_intent)

```

A partir de lo anterior podemos observar que la edad promedio de los solicitantes es de 28 años, concentrada entre 24 y 30, aunque aparecen valores extremos poco realistas como 144. Los ingresos muestran gran dispersión ya que la media ronda los 80.000, pero con un rango que va de 8.000 a más de 7 millones. Los préstamos solicitados se concentran alrededor de 9.500, con tasas de interés que oscilan entre 5,4% y 20% (media cercana al 11%). El `credit_score` varía entre 390 y 850, con promedio en 633.

La variable objetivo presenta un claro desbalance: 78% de préstamos rechazados frente a 22% aprobados. En cuanto a la intención del préstamo, destacan educación y gastos médicos como los motivos más frecuentes, mientras que mejoras en el hogar aparece como el menos común.

### Visualizaciones exploratorias

Para complementar el análisis, se generaron visualizaciones que permiten observar de manera más clara el comportamiento de las variables y su relación con el estado del préstamo.

```{r}
# Gráfico 1: Distribución de loan_status
ggplot(datos, aes(x = loan_status)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribución de estados del préstamo",
       x = "Estado", y = "Cantidad") +
  theme_minimal(base_size = 12)

```

El gráfico de barras muestra que la mayoría de los préstamos fueron rechazados (78%), mientras que solo el 22% fueron aprobados. Esta diferencia refleja cómo, en la práctica, las entidades suelen ser más cautelosas al otorgar créditos. Para nuestro análisis, este desbalance implica que no alcanza con medir la exactitud de un modelo, ya que podría sesgarse hacia la clase mayoritaria; por eso será necesario considerar otras métricas que reflejen mejor la capacidad de identificar los casos aprobados como la precisión, el recall o el F1-score.

```{r}
# Gráfico 2: Ingresos por loan_status
ggplot(datos, aes(x = loan_status, y = person_income)) +
  geom_boxplot(fill = "lightgreen", outlier.alpha = 0.3) +
  scale_y_continuous(labels = scales::label_number(big.mark = ".", decimal.mark = ",")) +
  labs(title = "Ingresos por estado del préstamo",
       x = "Estado del préstamo", y = "Ingreso anual") +
  theme_minimal(base_size = 12)

```

El boxplot permite comparar los ingresos de los solicitantes en función del estado del préstamo. Se observa que, en promedio, quienes obtuvieron aprobación tienden a tener ingresos algo más altos que los rechazados, aunque la diferencia no es tan marcada. Además, aparecen valores extremos muy elevados (outliers) que generan gran dispersión en ambos grupos.

##Ejercicio 3 **Construcción de un árbol de decisión básico**

Dividimos el conjunto de datos al azar en tres particiones: entrenamiento (70%), validación (15%) y testeo (15%). Utilizamos una semilla para asegurar la replicabilidad.

```{r}


#Guardamos la cantidad de muestras en dtos_totales:
datos_totales<- nrow(datos)

#calulamos la cantidad de muestras para cada conjunto de datos (test, train y validation):
tamano_entrenamiento <- 0.7 * datos_totales
tamano_validacion <- 0.15 * datos_totales
tamano_testeo <- datos_totales - tamano_entrenamiento - tamano_validacion

set.seed(0309)
indices <- sample(1:datos_totales)

entrenamiento_indices <- indices[1:tamano_entrenamiento]  # 70% de los datos
validacion_indices <- indices[(tamano_entrenamiento + 1):(tamano_entrenamiento + tamano_validacion)]  # 15% de los datos
testeo_indices <- indices[(tamano_entrenamiento + tamano_validacion + 1):datos_totales]  # 15% restantes

# Crear los tres conjuntos de datos
train_data <- datos[entrenamiento_indices, ]
valid_data <- datos[validacion_indices, ]
test_data <- datos[testeo_indices, ]
```

```{r}

tree <- rpart(formula = loan_status ~ person_age + person_gender + person_education + person_income + person_emp_exp + person_home_ownership + loan_amnt + loan_intent + loan_int_rate + loan_percent_income+ cb_person_cred_hist_length+ credit_score+ previous_loan_defaults_on_file, 
              data = train_data, 
              method = "class")
rpart.control()

```

El modelo de árbol se construyó con los valores por defecto de rpart. Estos hiperparámetros controlan el crecimiento del árbol. Por ejemplo, minsplit = 20 asegura que un nodo no se divida si tiene menos de 20 observaciones, mientras que cp = 0.01 evita que el modelo se vuelva excesivamente complejo al requerir que cada nueva división mejore al menos un 1% el ajuste. El valor de maxdepth = 30 es suficientemente grande como para no limitar el crecimiento en este caso.

Visualizamos el árbol:

```{r}
library(rpart.plot)

rpart.plot(tree)

 
```

### Interpretación del árbol de decisión

El árbol se construyó con `rpart()`, especificando como variable respuesta `loan_status` (Aprobado/Rechazado) y como predictoras distintas características del solicitante y del préstamo (edad, ingresos, historial crediticio, monto, tasa, etc.).

-   Los **colores** de los nodos indican la clase predicha (verde = *Aprobado*, azul = *Rechazado*).
-   La **intensidad** del color refleja la **pureza** del nodo (qué tan homogéneo es).
-   Dentro de cada nodo se ve la **proporción** de la clase mayoritaria y el **% de observaciones** que llegan a ese nodo.
-   Las **reglas** aparecen en las ramas y marcan la condición que manda cada observación a la izquierda o a la derecha.

**Lo que muestra nuestro árbol (primeros cortes):** - **Primer corte -- `previous_loan_defaults_on_file`:** es la variable más determinante. Si el solicitante tiene incumplimientos previos, el modelo lo clasifica directamente como **Rechazado**. - **Segundo corte -- `loan_percent_income` (\< 0.25):** entre quienes **no** tienen incumplimientos, la carga del préstamo sobre el ingreso es clave. Si la cuota representa **menos del 25%** del ingreso, la **probabilidad de aprobación** sube fuerte. - **Cortes posteriores -- `loan_int_rate`, `person_income`, `person_home_ownership`:** en general, **tasas más bajas**, **ingresos más altos** y **ser propietario** (u hipoteca) empujan hacia **Aprobado**.

Así el árbol prioriza señales de **riesgo histórico** (defaults previos) y de **capacidad de pago** (porcentaje del ingreso, tasa e ingresos), lo que es consistente con la lógica crediticia.

### Ejercicio 4

**Metricas de performance**

```{r}

# Predicción de clases
pred_clases <- predict(tree, newdata = test_data, type = "class")

# Predicción de probabilidades
pred_probabilidades <- predict(tree, newdata = test_data, type = "prob")

resultados <- data.frame(
  Clase_Predicha = pred_clases,
  Probabilidades = pred_probabilidades
)

head(resultados, 10)

```

Métricas de performance:

Importamos las librerías necesarias

```{r}
library(MLmetrics)
library(pROC)
```

Convertimos las predicciones y la variable objetivo en factores y guardamos la predicción del conjunto de testeo en una variable:

```{r}
test_data$loan_status <- as.factor(test_data$loan_status)
pred <- predict(tree, newdata = test_data, type = "class")
```

Matriz de confusión:

```{r}
conf_matrix <- confusionMatrix(pred, test_data$loan_status)
library(ggplot2)
conf_matrix$table

# Convertir a data.frame para graficar
df_matrix <- as.data.frame(conf_matrix$table)

# Gráfico
ggplot(df_matrix, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 6) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Matriz de Confusión", x = "Predicción", y = "Real") +
  theme_minimal()
```
El modelo es muy bueno detectando Rechazados (5019 vs 121) por lo que casi siempre que dice “Rechazado”, acierta. En cambio, es débil para detectar Aprobados (503 correctos vs 1107 errados).

Esto significa que el árbol favorece la clase que tiene mas cantidad de datos que en este caso es Rechazado.

Accuracy:

```{r}
accuracy <- Accuracy(pred, test_data$loan_status)
accuracy
```
El modelo acierta en alrededor del 91% de los casos del conjunto de test.

Como vimos en la matriz de confusión, el rendimiento para identificar la clase Aprobado es mas bajo. Por lo que concluimos en que el valor del accuracy está condicionado. La mayoría de las observaciones son Rechazado, y el modelo tiende a predecir esa clase. 

Precision y Recall:

```{r}
precision_val <- Precision(test_data$loan_status,pred,positive = NULL)
recall_val <-Recall(test_data$loan_status,pred,positive = NULL)

precision_val
recall_val
```
Precision es aproximadamente 0.91. Significa que de todos los casos que el modelo predijo como positivos, alrededor del 91% realmente lo eran.

Recall es aproximadamente 0.97. Significa que de los que realmente son positivos, el modelo detecta el 97,6%. 

El modelo tomó como positivos a los Rechazados. Es por esto que ambos valores son muy altos. 

F1-score

```{r}
f_1_score<- F1_Score(test_data$loan_status,pred,positive = NULL)
f_1_score
```
El F1-Score es la media armónica entre precision y recall. Esto indica que no solo predice la mayoría de los rechazados reales, sino que tambien mantiene bajo el número de falsos rechazados. 

AUC-ROC

```{r}
probabilities <- predict(tree, newdata = test_data, type = "prob")

auc_value <- MLmetrics::AUC(y_pred = probabilities[, "Aprobado"], y_true = as.numeric(test_data$loan_status == "Aprobado"))
print(paste("AUC:", auc_value))
```
Significa que el modelo diferencia bien entre “Aprobado” y “Rechazado”. Por lo que el rendimiento es bueno.


```{r}

random_search <- function(train_data, valid_data, n) {
  # asegurar niveles (negativo, positivo)
  lvl <- c("Aprobado", "Rechazado")
  train_data$loan_status <- factor(train_data$loan_status, levels = lvl)
  valid_data$loan_status   <- factor(valid_data$loan_status, levels = lvl)
  
  # resultados
  results <- data.frame(
    maxdepth = integer(),
    minsplit = integer(),
    minbucket = integer(),
    AUC_Validation = numeric(),
    stringsAsFactors = FALSE
  )
  
  set.seed(777) # replicabilidad
  
  for (i in 1:n) {
    # hiperparámetros aleatorios
    maxdepth  <- sample(2:30, 1)
    minsplit  <- sample(10:500, 1)
    minbucket <- max(1L, round(minsplit / 3)) 
    
    
    # entrenar con cp=0 y xval=0 
    tree_model <- suppressMessages(suppressWarnings(
      rpart(loan_status ~ .,
            data = train_data,
            method = "class",
            control = rpart.control(minsplit = minsplit,
                                    minbucket = minbucket,
                                    cp = 0, xval = 0,
                                    maxdepth = maxdepth))
    ))
    
    # probas en validación (tomamos la columna "Aprobado" por NOMBRE)
    probability_pred <- predict(tree_model, newdata = valid_data, type = "prob")
    prob_pos <- probability_pred[, "Rechazado"]
    
    # AUC en validación (con levels explícitos)
    roc_curve <- roc(response = valid_data$loan_status, predictor = prob_pos,
                     levels = lvl, quiet = TRUE)
    auc_value <- as.numeric(auc(roc_curve))
    
    # guardar
    results <- rbind(results, data.frame(maxdepth, minsplit, minbucket,
                                         AUC_Validation = auc_value))
  }
  
  # mejor por AUC
  best_model <- results[which.max(results$AUC_Validation), , drop = FALSE]
  return(list(best_model = best_model, results = results))
}

```

```{r}
library(rpart); library(pROC)
rs <- random_search(train_data, valid_data, n = 1000)  
rs$best_model    # hiperparámetros y AUC de validación del mejor
```

### Visualización de la relación entre hiperparámetros y AUC-ROC  

Para evaluar cómo influyen los hiperparámetros en la calidad del modelo, generamos gráficos que muestran la relación de *maxdepth, **minsplit* y *minbucket* con el *AUC-ROC obtenido en validación*.  
Estos permiten identificar patrones o tendencias que orientan la selección de configuraciones más adecuadas.

```{r}
library(tidyr)

plot_data <- rs$results %>%
  pivot_longer(cols = c("maxdepth", "minsplit", "minbucket"),
               names_to = "Hiperparámetro", values_to = "Valor")

ggplot(plot_data, aes(x = Valor, y = AUC_Validation)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_smooth(method = "loess", se = FALSE, color = "darkred", size = 1) +
  facet_wrap(~ Hiperparámetro, scales = "free_x") +
  labs(title = "Impacto de los hiperparámetros en el AUC-ROC (validación)",
       x = "Valor del hiperparámetro",
       y = "AUC-ROC") +
  theme_minimal(base_size = 13)
```


```{r}
best_tree <- rpart(
  loan_status ~ ., 
  data = rbind(train_data,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs$best_model$minsplit,
    minbucket = rs$best_model$minbucket,
    maxdepth  = rs$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

prob_best <- predict(best_tree, newdata = test_data, type = "prob")[, "Rechazado"]

```

```{r}

lvl <- c("Aprobado", "Rechazado")
train_data$loan_status <- factor(train_data$loan_status, levels = lvl)
valid_data$loan_status <- factor(valid_data$loan_status, levels = lvl)
test_data$loan_status   <- factor(test_data$loan_status, levels = lvl)

# entrenar con los hiperparámetros encontrados
best_tree <- rpart(
  loan_status ~ ., 
  data = rbind(train_data,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs$best_model$minsplit,
    minbucket = rs$best_model$minbucket,
    maxdepth  = rs$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

# predecir probabilidades en test
probas_test <- predict(best_tree, newdata = test_data, type = "prob")[, "Rechazado"]

# calcular curva ROC y AUC
roc_test <- roc(response = test_data$loan_status,
                predictor = probas_test,
                levels = lvl,
                quiet = TRUE)

auc_test <- auc(roc_test)
auc_test

```


AGREGAR COMPARACION ENTRE ARBOL DEL EJ3

##ejercicio 5
rpart.plot(tree)

```{r}
rpart.plot(best_tree)
```



```{r}
best_tree$control$minsplit
best_tree$control$minbucket
best_tree$control$maxdepth
```


